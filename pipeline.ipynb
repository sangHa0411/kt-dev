{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from datasets import DatasetDict, Dataset\n",
    "from models.model import T5EncoderModel\n",
    "from utils.metrics import NERMetrics\n",
    "from utils.loader import Loader\n",
    "from utils.parser import NERParser\n",
    "from utils.seperate import Spliter\n",
    "from utils.encoder import NEREncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5TokenizerFast,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load datasets\n"
     ]
    }
   ],
   "source": [
    " # -- Loading datasets\n",
    "print(\"\\nLoad datasets\")\n",
    "loader = Loader(\"data\", \"klue_ner_test_20.txt\")\n",
    "raw_dataset = loader.load(test_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['초반에 약간 뭐하는거지 할수는있는데뒤로갈수록 몰입도장난아님 주제도좋고 진짜편집좋다 <앤드류거필드:PS>도좋고',\n",
       " '국내 업체들이 마진이 적다는 이유로 경차 개발을 꺼린 탓에 경차가 자동차 시장 전체에서 차지하는 비중은 <10%:QT>를 간신히 넘는 상황이다.',\n",
       " '역시 <한지원:PS> 감독님.!코피루왁에서 눈물흘리면서 힐링 잘했습니다']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5201/5201 [00:00<00:00, 84085.58it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = []\n",
    "\n",
    "for i in tqdm(range(len(raw_dataset))) :\n",
    "    data = raw_dataset[i]\n",
    "    info = re.search(\"<[^:]+:[A-Z]{2}>\", data)\n",
    "\n",
    "    while info is not None :\n",
    "        start_p, end_p = info.span()\n",
    "        group = info.group()\n",
    "        tag_word, tag_name = group[1:-1].split(':')\n",
    "        \n",
    "        data = data[:start_p] + tag_word + data[end_p:]\n",
    "        info = re.search(\"<[^:]+:[A-Z]{2}>\", data)\n",
    "\n",
    "    data = re.sub(\" {1,}\", \" \", data).strip()\n",
    "    raw_sentences.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29일 금융권에 따르면 금융위원회와 금융감독원은 설 연휴가 끝난 직후인 2월 초에 대규모 인력을 투입해 신한카드, 삼성카드, 현대카드, 하나SK카드, 우리카드, 비씨카드 등 6개 전업 카드사에 대한 현장 검사를 실시한다.\n"
     ]
    }
   ],
   "source": [
    "rand_id = np.random.randint(len(raw_sentences))\n",
    "rand_sen = raw_sentences[rand_id]\n",
    "print(rand_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"sentences\" : raw_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>초반에 약간 뭐하는거지 할수는있는데뒤로갈수록 몰입도장난아님 주제도좋고 진짜편집좋다 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>국내 업체들이 마진이 적다는 이유로 경차 개발을 꺼린 탓에 경차가 자동차 시장 전체...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>역시 한지원 감독님.!코피루왁에서 눈물흘리면서 힐링 잘했습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>슬라이가 너무 평법한 액션 스릴러로 풀어버린 작품이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일본 애니메이션 진격의 거인을 연상시키는 진격의 농부가 등장해 각종 커뮤니티 사이트...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  초반에 약간 뭐하는거지 할수는있는데뒤로갈수록 몰입도장난아님 주제도좋고 진짜편집좋다 ...\n",
       "1  국내 업체들이 마진이 적다는 이유로 경차 개발을 꺼린 탓에 경차가 자동차 시장 전체...\n",
       "2                 역시 한지원 감독님.!코피루왁에서 눈물흘리면서 힐링 잘했습니다\n",
       "3                     슬라이가 너무 평법한 액션 스릴러로 풀어버린 작품이다.\n",
       "4  일본 애니메이션 진격의 거인을 연상시키는 진격의 농부가 등장해 각종 커뮤니티 사이트..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentences'],\n",
       "    num_rows: 5201\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file exps/ner/fold-0/added_tokens.json. We won't load it.\n",
      "loading file exps/ner/fold-0/spiece.model\n",
      "loading file exps/ner/fold-0/tokenizer.json\n",
      "loading file None\n",
      "loading file exps/ner/fold-0/special_tokens_map.json\n",
      "loading file exps/ner/fold-0/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "PLM = \"exps/ner/fold-0\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(PLM, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = NEREncoder(tokenizer, max_length=128, label_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd50ef402089472091ef4a3af32f84c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(encoder, batched=True, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5201\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file exps/ner/fold-0/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"KETI-AIR/ke-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5EncoderModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label_size\": 7,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64128\n",
      "}\n",
      "\n",
      "loading weights file exps/ner/fold-0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5EncoderModel.\n",
      "\n",
      "All the weights of T5EncoderModel were initialized from the model checkpoint at exps/ner/fold-0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5EncoderModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = T5Config.from_pretrained(PLM)\n",
    "model = T5EncoderModel.from_pretrained(PLM, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,        \n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5201\n",
      "  Batch size = 8\n",
      "/home/wkrtkd911/anaconda3/envs/kt-dev/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='651' max='651' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [651/651 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.predict(test_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_size, tag_size = results[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = np.argmax(results[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = tokenizer(\n",
    "    raw_sentences,\n",
    "    return_token_type_ids=False,\n",
    "    return_offsets_mapping=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "offsets = tokenized_sentences.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(data, tokenizer) :\n",
    "\n",
    "    i = 0\n",
    "    pos_label = []\n",
    "    pos_data = tokenizer.pos(data)\n",
    "\n",
    "    for pos in pos_data :\n",
    "        word, tag = pos\n",
    "\n",
    "        for j in range(len(word)) :\n",
    "            pos_label.append(tag[0])\n",
    "            i += 1\n",
    "        \n",
    "        if i < len(data) and data[i] == \" \" :\n",
    "            pos_label.append(\"U\")\n",
    "            i += 1\n",
    "\n",
    "    return pos_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5201/5201 [00:01<00:00, 4602.79it/s]\n"
     ]
    }
   ],
   "source": [
    "postprocessed_words = []\n",
    "postprocessed_tags = []\n",
    "\n",
    "for i in tqdm(range(len(tags))) :\n",
    "\n",
    "    tag = tags[i]\n",
    "    offset = offsets[i]\n",
    "    sentence = raw_sentences[i]\n",
    "    pos = get_pos(sentence, mecab)\n",
    "\n",
    "    char_list = list(sentence)\n",
    "    label_list = []\n",
    "\n",
    "    if offset[0][0] == offset[1][0] :\n",
    "        offset = offset[1:]\n",
    "\n",
    "    for j in range(len(offset)-1) :\n",
    "        start_p, end_p = offset[j]\n",
    "        label = tag[j]\n",
    "        label_list.extend([label] * (end_p - start_p))\n",
    "    label_list.append(0)\n",
    "\n",
    "    # 문제상황\n",
    "    prev = 0\n",
    "    k = 1\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "    while k < len(label_list) :\n",
    "\n",
    "        if label_list[k] != label_list[k-1] :\n",
    "            if label_list[k-1] > 0 :\n",
    "                \n",
    "                l = k-1\n",
    "                while pos[l] == \"J\" :\n",
    "                    l -= 1\n",
    "\n",
    "                word = [char_list[j] for j in range(prev, l+1)]\n",
    "                word = \"\".join(word).strip()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(label_list[l])\n",
    "            prev = k\n",
    "        k += 1\n",
    "\n",
    "    postprocessed_words.append(word_list)\n",
    "    postprocessed_tags.append(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152\n",
      "<육군:OG>은 <24일:DT> 인터넷 홈페이지에 내가 아는 선배, 친구, 가족들이 군대에 가면 ~~카더라, 진실은이라는 제목의 흥미로운 글을 올렸다.\n",
      "육군은 24일 인터넷 홈페이지에 내가 아는 선배, 친구, 가족들이 군대에 가면 ~~카더라, 진실은이라는 제목의 흥미로운 글을 올렸다.\n",
      "['육군', '24일']\n",
      "[6, 2]\n"
     ]
    }
   ],
   "source": [
    "rand_id = np.random.randint(len(postprocessed_words))\n",
    "print(rand_id)\n",
    "print(raw_dataset[rand_id])\n",
    "print(raw_sentences[rand_id])\n",
    "print(postprocessed_words[rand_id])\n",
    "print(postprocessed_tags[rand_id])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94760e65e4dad52476f5bb39f49d6da82a9446ce708d0315afad0611d9f08a06"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('etri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
